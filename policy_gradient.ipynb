{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d28f0a8",
   "metadata": {},
   "source": [
    "# Get VOC 2007 train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bd62ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ba27ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_voc_dataset(path, year):\n",
    "    T = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(),])\n",
    "    voc_data =  torchvision.datasets.VOCDetection(path, year=year, image_set='train', transform=T, download=True)\n",
    "    voc_val =  torchvision.datasets.VOCDetection(path, year=year, image_set='val', transform=T, download=True)\n",
    "    return voc_data, voc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca1003a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: .\\VOCtrainval_06-Nov-2007.tar\n",
      "Extracting .\\VOCtrainval_06-Nov-2007.tar to .\n",
      "Using downloaded and verified file: .\\VOCtrainval_06-Nov-2007.tar\n",
      "Extracting .\\VOCtrainval_06-Nov-2007.tar to .\n"
     ]
    }
   ],
   "source": [
    "train_loader2007, val_loader2007 = read_voc_dataset(path=\".\" ,year='2007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a4ad800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset VOCDetection\n",
       "    Number of datapoints: 2501\n",
       "    Root location: .\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2948e03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2667, 0.2588, 0.2588,  ..., 0.2314, 0.2196, 0.2078],\n",
       "          [0.2667, 0.2588, 0.2588,  ..., 0.2431, 0.2314, 0.2235],\n",
       "          [0.2667, 0.2667, 0.2667,  ..., 0.2471, 0.2471, 0.2471],\n",
       "          ...,\n",
       "          [0.2980, 0.3020, 0.3098,  ..., 0.3333, 0.3176, 0.3137],\n",
       "          [0.3098, 0.3098, 0.3176,  ..., 0.3216, 0.3216, 0.3216],\n",
       "          [0.3216, 0.3216, 0.3216,  ..., 0.3137, 0.3137, 0.3098]],\n",
       " \n",
       "         [[0.2667, 0.2588, 0.2588,  ..., 0.2235, 0.2235, 0.2157],\n",
       "          [0.2667, 0.2588, 0.2588,  ..., 0.2353, 0.2353, 0.2314],\n",
       "          [0.2667, 0.2667, 0.2667,  ..., 0.2431, 0.2549, 0.2510],\n",
       "          ...,\n",
       "          [0.2941, 0.2941, 0.3020,  ..., 0.3176, 0.3176, 0.3137],\n",
       "          [0.3059, 0.3059, 0.3098,  ..., 0.3176, 0.3216, 0.3216],\n",
       "          [0.3176, 0.3176, 0.3137,  ..., 0.3098, 0.3137, 0.3098]],\n",
       " \n",
       "         [[0.2588, 0.2549, 0.2588,  ..., 0.2275, 0.2235, 0.2118],\n",
       "          [0.2588, 0.2549, 0.2588,  ..., 0.2392, 0.2353, 0.2275],\n",
       "          [0.2588, 0.2627, 0.2667,  ..., 0.2431, 0.2510, 0.2510],\n",
       "          ...,\n",
       "          [0.2824, 0.2902, 0.3059,  ..., 0.3137, 0.3098, 0.3059],\n",
       "          [0.2902, 0.3020, 0.3137,  ..., 0.3098, 0.3137, 0.3137],\n",
       "          [0.3020, 0.3098, 0.3176,  ..., 0.3098, 0.3059, 0.3020]]]),\n",
       " {'annotation': {'folder': 'VOC2007',\n",
       "   'filename': '000012.jpg',\n",
       "   'source': {'database': 'The VOC2007 Database',\n",
       "    'annotation': 'PASCAL VOC2007',\n",
       "    'image': 'flickr',\n",
       "    'flickrid': '207539885'},\n",
       "   'owner': {'flickrid': 'KevBow', 'name': '?'},\n",
       "   'size': {'width': '500', 'height': '333', 'depth': '3'},\n",
       "   'segmented': '0',\n",
       "   'object': [{'name': 'car',\n",
       "     'pose': 'Rear',\n",
       "     'truncated': '0',\n",
       "     'difficult': '0',\n",
       "     'bndbox': {'xmin': '156', 'ymin': '97', 'xmax': '351', 'ymax': '270'}}]}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader2007[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20472b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset VOCDetection\n",
       "    Number of datapoints: 2510\n",
       "    Root location: .\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader2007"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a4aab",
   "metadata": {},
   "source": [
    "# Sort Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6f6b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.notebook as tq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde06281",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['cat', 'cow', 'dog', 'bird', 'car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b67f4eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_class_extract(datasets):\n",
    "    datasets_per_class = {}\n",
    "    \n",
    "    \n",
    "    for j in classes:\n",
    "        datasets_per_class[j] = {}\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for i in tq.tqdm(dataset):\n",
    "            \n",
    "            img, target = i\n",
    "            obj = target['annotation']['object']\n",
    "            \n",
    "            if isinstance(obj, list):\n",
    "                classe = target['annotation']['object'][0][\"name\"]\n",
    "            else:\n",
    "                classe = target['annotation']['object'][\"name\"]\n",
    "                \n",
    "            filename = target['annotation']['filename']\n",
    "\n",
    "            org = {}\n",
    "            \n",
    "            for j in classes:\n",
    "                org[j] = []\n",
    "                org[j].append(img)\n",
    "            \n",
    "            if isinstance(obj, list):\n",
    "                for j in range(len(obj)):\n",
    "                    classe = obj[j][\"name\"]\n",
    "                    if classe in classes:\n",
    "                        org[classe].append([obj[j][\"bndbox\"], target['annotation']['size']])\n",
    "            else:\n",
    "                if classe in classes:\n",
    "                    org[classe].append([obj[\"bndbox\"], target['annotation']['size']])\n",
    "                    \n",
    "            for j in classes:\n",
    "                if len(org[j]) > 1:\n",
    "                    try:\n",
    "                        datasets_per_class[j][filename].append(org[j])\n",
    "                    except KeyError:\n",
    "                        datasets_per_class[j][filename] = []\n",
    "                        datasets_per_class[j][filename].append(org[j])\n",
    "                        \n",
    "    return datasets_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e435ab9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0baf77abe65143369dcde89970ff91ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets_per_class = sort_class_extract([train_loader2007])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c79a2045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee2e0603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6b18df3b08448eaa63da5644b64f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe cat...\n",
      "166\n",
      "Classe cow...\n",
      "71\n",
      "Classe dog...\n",
      "210\n",
      "Classe bird...\n",
      "182\n",
      "Classe car...\n",
      "402\n"
     ]
    }
   ],
   "source": [
    "classes = ['cat', 'cow', 'dog', 'bird', 'car']\n",
    "\n",
    "img_num = 0\n",
    "\n",
    "for i in tq.tqdm(range(len(classes))):\n",
    "    classe = classes[i]\n",
    "    print(\"Classe \" + str(classe) + \"...\")\n",
    "    print(len(datasets_per_class[classe]))\n",
    "    # print(datasets_per_class[classe])\n",
    "    \n",
    "    img_num += len(datasets_per_class[classe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aab1e401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1031"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2cc068",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f39cf4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "# import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dabb3dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, network='vgg16'):\n",
    "        print(network)\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        if network == 'vgg16':\n",
    "            model = torchvision.models.vgg16(pretrained=True)\n",
    "        elif network == 'resnet50':\n",
    "            model = torchvision.models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            model = torchvision.models.alexnet(pretrained=True)\n",
    "        model.eval() # to not do dropout\n",
    "        self.features = list(model.children())[0]\n",
    "        if network == 'vgg16':\n",
    "            self.classifier = nn.Sequential(*list(model.classifier.children())[:-2])\n",
    "        else:\n",
    "            self.classifier = nn.Sequential(*list(model.children())[:-2])\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "494484e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy_net, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=81 + 25088, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=512, out_features=9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.softmax(self.classifier(x),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0d04b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(index, loader):\n",
    "    extracted = loader[index]\n",
    "    ground_truth_boxes =[]\n",
    "    \n",
    "    for ex in extracted:\n",
    "        img = ex[0]\n",
    "        bndbox = ex[1][0]\n",
    "        size = ex[1][1]\n",
    "        \n",
    "        xmin = (float(bndbox['xmin']) /  float(size['width'])) * 224\n",
    "        xmax = (float(bndbox['xmax']) /  float(size['width'])) * 224\n",
    "\n",
    "        ymin = (float(bndbox['ymin']) /  float(size['height'])) * 224\n",
    "        ymax = (float(bndbox['ymax']) /  float(size['height'])) * 224\n",
    "\n",
    "        ground_truth_boxes.append([xmin, xmax, ymin, ymax])\n",
    "        \n",
    "    return img, ground_truth_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7af6604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image, feature_extractor, dtype=FloatTensor):\n",
    "    global transform\n",
    "    image = image.view(1, *image.shape)\n",
    "    image = Variable(image).type(dtype)\n",
    "    # if use_cuda:\n",
    "        # image = image.cuda()\n",
    "\n",
    "    feature = feature_extractor(image)\n",
    "    # print(\"Feature shape: \" + str(feature.shape))\n",
    "\n",
    "    return feature.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "85df0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_state(image, actions_history, feature_extractor, dtype=FloatTensor):\n",
    "    image_feature = get_features(image, feature_extractor, dtype)\n",
    "    image_feature = image_feature.view(1,-1)\n",
    "\n",
    "    # print(\"image feature: \" + str(image_feature.shape))\n",
    "\n",
    "    history_flatten = actions_history.view(1,-1).type(dtype)\n",
    "\n",
    "    state = torch.cat((image_feature, history_flatten), 1)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "da9f5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, policy_net):\n",
    "    probs = policy_net(state)\n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4beded50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrap(coord):\n",
    "    return min(max(coord, 0.0), 224.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0b013f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_position_box(actions, xmin=0.0, xmax=224.0, ymin=0.0, ymax=224.0):\n",
    "    alpha = 0.1\n",
    "    \n",
    "    alpha_h = alpha * (ymax - ymin)\n",
    "    alpha_w = alpha * (xmax - xmin)\n",
    "    \n",
    "    real_x_min, real_x_max, real_y_min, real_y_max = 0, 224, 0, 224\n",
    "\n",
    "    for r in actions:\n",
    "        if r == 1: # Right\n",
    "            real_x_min += alpha_w\n",
    "            real_x_max += alpha_w\n",
    "        if r == 2: # Left\n",
    "            real_x_min -= alpha_w\n",
    "            real_x_max -= alpha_w\n",
    "        if r == 3: # Up \n",
    "            real_y_min -= alpha_h\n",
    "            real_y_max -= alpha_h\n",
    "        if r == 4: # Down\n",
    "            real_y_min += alpha_h\n",
    "            real_y_max += alpha_h\n",
    "        if r == 5: # Bigger\n",
    "            real_y_min -= alpha_h\n",
    "            real_y_max += alpha_h\n",
    "            real_x_min -= alpha_w\n",
    "            real_x_max += alpha_w\n",
    "        if r == 6: # Smaller\n",
    "            real_y_min += alpha_h\n",
    "            real_y_max -= alpha_h\n",
    "            real_x_min += alpha_w\n",
    "            real_x_max -= alpha_w\n",
    "            \n",
    "            if real_y_min >= real_y_max:\n",
    "                real_y_min -= alpha_h\n",
    "                real_y_max += alpha_h\n",
    "                \n",
    "            if real_x_min >= real_x_max:\n",
    "                real_x_min -= alpha_w\n",
    "                real_x_max += alpha_w\n",
    "            \n",
    "        if r == 7: # Fatter\n",
    "            real_y_min += alpha_h\n",
    "            real_y_max -= alpha_h\n",
    "            \n",
    "            if real_y_min >= real_y_max:\n",
    "                real_y_min -= alpha_h\n",
    "                real_y_max += alpha_h\n",
    "                \n",
    "        if r == 8: # Taller\n",
    "            real_x_min += alpha_w\n",
    "            real_x_max -= alpha_w\n",
    "            \n",
    "            if real_x_min >= real_x_max:\n",
    "                real_x_min -= alpha_w\n",
    "                real_x_max += alpha_w\n",
    "            \n",
    "    real_x_min, real_x_max, real_y_min, real_y_max = rewrap(real_x_min), rewrap(real_x_max), rewrap(real_y_min), rewrap(real_y_max)\n",
    "    \n",
    "    if real_x_max == 0.0:\n",
    "        real_x_max += alpha_w\n",
    "        \n",
    "    if real_x_min == 224.0:\n",
    "        real_x_min -= alpha_w\n",
    "        \n",
    "    if real_y_max == 0.0:\n",
    "        real_y_max += alpha_h\n",
    "        \n",
    "    if real_y_min == 224.0:\n",
    "        real_y_min -= alpha_h\n",
    "    \n",
    "    return [real_x_min, real_x_max, real_y_min, real_y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e5b1d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(box1, box2):\n",
    "    x11, x21, y11, y21 = box1\n",
    "    x12, x22, y12, y22 = box2\n",
    "\n",
    "    yi1 = max(y11, y12)\n",
    "    xi1 = max(x11, x12)\n",
    "    yi2 = min(y21, y22)\n",
    "    xi2 = min(x21, x22)\n",
    "    inter_area = max(((xi2 - xi1) * (yi2 - yi1)), 0)\n",
    "    box1_area = (x21 - x11) * (y21 - y11)\n",
    "    box2_area = (x22 - x12) * (y22 - y12)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "92d3371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_bdbox(ground_truth_boxes, coord):\n",
    "    max_iou = False\n",
    "    max_gt = []\n",
    "    for ground_truth in ground_truth_boxes:\n",
    "        iou = intersection_over_union(coord, ground_truth)\n",
    "        if max_iou == False or max_iou < iou:\n",
    "            max_iou = iou\n",
    "            max_ground_truth = ground_truth\n",
    "    return max_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "374e2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trigger_reward(actual_state, ground_truth):\n",
    "    res = intersection_over_union(actual_state, ground_truth)\n",
    "    # if res>=self.threshold:\n",
    "        # return self.nu\n",
    "    # return -1*self.nu\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2b1fa02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_history(action, actions_history):\n",
    "    action_vector = torch.zeros(9)\n",
    "    action_vector[action] = 1\n",
    "    size_history_vector = len(torch.nonzero(actions_history))\n",
    "    \n",
    "    # print(torch.nonzero(actions_history))\n",
    "    # print(size_history_vector)\n",
    "    \n",
    "    if size_history_vector < 9:\n",
    "        actions_history[size_history_vector][action] = 1\n",
    "    else:\n",
    "        for i in range(8, 0, -1):\n",
    "            actions_history[i][:] = actions_history[i-1][:]\n",
    "        actions_history[0][:] = action_vector[:] \n",
    "        \n",
    "    # print(actions_history)\n",
    "    \n",
    "    return actions_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a6616df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0.0\n",
    "xmax = 224.0\n",
    "ymin = 0.0\n",
    "ymax = 224.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "62e97a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946957b9849d4785b92b68d92b27c2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe cat...\n",
      "vgg16\n",
      "Episode 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msi-\\AppData\\Local\\Temp/ipykernel_21384/1078738684.py:122: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state_list[i]).to(device)\n",
      "C:\\Users\\msi-\\AppData\\Local\\Temp/ipykernel_21384/1078738684.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action = torch.tensor(action_list[i]).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1013.1519567798823\n",
      "Episode 1\n",
      "1039.239423070103\n",
      "Episode 2\n",
      "1180.7061550319195\n",
      "Episode 3\n",
      "1248.5002516228706\n",
      "Episode 4\n",
      "1277.7404329078272\n"
     ]
    }
   ],
   "source": [
    "for i in tq.tqdm(range(len(classes))):\n",
    "    classe = classes[i]\n",
    "    print(\"Classe \" + str(classe) + \"...\")\n",
    "    \n",
    "    num_episodes=5\n",
    "    model_name='vgg16'\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    feature_extractor = FeatureExtractor(network=model_name)\n",
    "    feature_extractor.cuda()\n",
    "    \n",
    "    policy_net = Policy_net()\n",
    "    policy_net.cuda()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-6)\n",
    "    # optimizer.cuda()\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        print(\"Episode\" + \" \" + str(i_episode))\n",
    "        \n",
    "        cnt = 0\n",
    "        \n",
    "        sum_loss = 0.0\n",
    "        \n",
    "        for key, _ in datasets_per_class[classe].items():\n",
    "            cnt += 1\n",
    "            # print(\"item: \", cnt)\n",
    "            \n",
    "            image, ground_truth_boxes = extract(key, datasets_per_class[classe])\n",
    "            # print(image)\n",
    "            # print(ground_truth_boxes)\n",
    "            \n",
    "            original_image = image.clone()\n",
    "            ground_truth = ground_truth_boxes[0]\n",
    "            \n",
    "            all_actions = []\n",
    "            \n",
    "            actions_history = torch.ones((9,9))\n",
    "            state = compose_state(image, actions_history, feature_extractor)\n",
    "            \n",
    "            original_coordinates = [xmin, xmax, ymin, ymax]\n",
    "            new_image = image\n",
    "            done = False\n",
    "            \n",
    "            t = 0 #timer\n",
    "            \n",
    "            actual_equivalent_coord = original_coordinates\n",
    "            \n",
    "            transition_dict = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            \n",
    "            while not done:\n",
    "                t += 1\n",
    "                action = choose_action(state, policy_net)\n",
    "                # print(action)\n",
    "                all_actions.append(action)\n",
    "                \n",
    "                if action == 0:\n",
    "                    # print(\"yes\")\n",
    "                    next_state = None\n",
    "                    new_equivalent_coord = calculate_position_box(all_actions)\n",
    "                    # print(new_equivalent_coord)\n",
    "                    closest_ground_box = get_max_bdbox(ground_truth_boxes, new_equivalent_coord)\n",
    "                    # print(closest_ground_box)\n",
    "                    reward = compute_trigger_reward(new_equivalent_coord, closest_ground_box)\n",
    "                    # print(reward)\n",
    "                    done = True\n",
    "                    \n",
    "                else:\n",
    "                    # print(\"no\")\n",
    "                    # next_state = None\n",
    "                    \n",
    "                    actions_history = update_history(action, actions_history)\n",
    "                    \n",
    "                    new_equivalent_coord = calculate_position_box(all_actions)\n",
    "                    # print(new_equivalent_coord)\n",
    "                    \n",
    "                    new_image = original_image[:, int(new_equivalent_coord[2]):int(new_equivalent_coord[3]), int(new_equivalent_coord[0]):int(new_equivalent_coord[1])]\n",
    "                    new_image = transform(new_image)\n",
    "                    next_state = compose_state(new_image, actions_history, feature_extractor)\n",
    "                    \n",
    "                    closest_ground_box = get_max_bdbox(ground_truth_boxes, new_equivalent_coord)\n",
    "                    # print(closest_ground_box)\n",
    "                    reward = compute_trigger_reward(new_equivalent_coord, closest_ground_box)\n",
    "                    # print(reward)\n",
    "                    \n",
    "                    actual_equivalent_coord = new_equivalent_coord\n",
    "                    \n",
    "                if t == 20:\n",
    "                    done = True\n",
    "                    \n",
    "                transition_dict['states'].append(state)\n",
    "                transition_dict['actions'].append(action)\n",
    "                transition_dict['next_states'].append(next_state)\n",
    "                transition_dict['rewards'].append(reward)\n",
    "                transition_dict['dones'].append(done)\n",
    "                \n",
    "                # print(transition_dict)\n",
    "                \n",
    "                state = next_state\n",
    "                image = new_image\n",
    "                \n",
    "                # break\n",
    "                \n",
    "            reward_list = transition_dict['rewards']\n",
    "            state_list = transition_dict['states']\n",
    "            action_list = transition_dict['actions']\n",
    "            \n",
    "            G = 0\n",
    "            gamma = 0.98\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            for i in reversed(range(len(reward_list))):\n",
    "                reward = reward_list[i]\n",
    "                state = torch.tensor(state_list[i]).to(device)\n",
    "                action = torch.tensor(action_list[i]).to(device)\n",
    "                \n",
    "                log_prob = torch.log(policy_net(state))\n",
    "                \n",
    "                # print(log_prob)\n",
    "                \n",
    "                # probs = policy_network(state)\n",
    "                \n",
    "                # m = Categorical(probs)\n",
    "                # action = m.sample()\n",
    "                # next_state, reward = env.step(action)\n",
    "                # loss = -m.log_prob(action) * reward\n",
    "                \n",
    "                probs = policy_net(state)\n",
    "                m = torch.distributions.Categorical(probs)\n",
    "                # action = m.sample()\n",
    "                \n",
    "                # print(m.log_prob(action))\n",
    "\n",
    "                # break\n",
    "                                  \n",
    "                G = gamma * G + reward\n",
    "                loss = -m.log_prob(action) * G\n",
    "                \n",
    "                if i == 0:\n",
    "                    # print(loss)\n",
    "                    sum_loss += loss.item()\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "            optimizer.step()\n",
    "            \n",
    "        print(sum_loss)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822a9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3821583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dc74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy_net, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=81 + 25088, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=512, out_features=9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.classifier(x),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659cab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, classe, num_episodes=15, model_name='vgg16'):\n",
    "        self.BATCH_SIZE = 100\n",
    "        self.discount_factor = 0.95\n",
    "        self.model_name = model_name\n",
    "        screen_height = 224\n",
    "        screen_width = 224\n",
    "        self.n_actions = 9\n",
    "        self.classe = classe\n",
    "        \n",
    "        self.feature_extractor = FeatureExtractor(network=self.model_name)\n",
    "        \n",
    "        self.policy_net = Policy_net()\n",
    "        \n",
    "        self.actions_history = []\n",
    "        self.num_episodes = num_episodes\n",
    "        \n",
    "    def train(self, train_loader):\n",
    "        xmin = 0.0\n",
    "        xmax = 224.0\n",
    "        ymin = 0.0\n",
    "        ymax = 224.0\n",
    "        \n",
    "        for i_episode in range(self.num_episodes):\n",
    "            print(\"Episode\" + \" \" + str(i_episode))\n",
    "            \n",
    "            for key, _ in train_loader.items():\n",
    "                image, ground_truth_boxes = extract(key, train_loader)\n",
    "                original_image = image.clone()\n",
    "                ground_truth = ground_truth_boxes[0]\n",
    "                all_actions = []\n",
    "                \n",
    "            self.actions_history = torch.ones((9,9))\n",
    "            state = self.compose_state(image)\n",
    "            \n",
    "            original_coordinates = [xmin, xmax, ymin, ymax]\n",
    "            new_image = image\n",
    "            done = False\n",
    "            t = 0\n",
    "            \n",
    "            actual_equivalent_coord = original_coordinates\n",
    "            \n",
    "            while not done:\n",
    "                t += 1\n",
    "                \n",
    "            \n",
    "    def compose_state(self, image, dtype=FloatTensor):\n",
    "        image_feature = self.get_features(image, dtype)\n",
    "        image_feature = image_feature.view(1,-1)\n",
    "        print(\"image feature: \" + str(image_feature.shape))\n",
    "\n",
    "        history_flatten = self.actions_history.view(1,-1).type(dtype)\n",
    "\n",
    "        state = torch.cat((image_feature, history_flatten), 1)\n",
    "        return state\n",
    "    \n",
    "    def get_features(self, image, dtype=FloatTensor):\n",
    "        global transform\n",
    "        image = image.view(1, *image.shape)\n",
    "        image = Variable(image).type(dtype)\n",
    "        if use_cuda:\n",
    "            image = image.cuda()\n",
    "            \n",
    "        feature = self.feature_extractor(image)\n",
    "        print(\"Feature shape: \" + str(feature.shape))\n",
    "        \n",
    "        return feature.data\n",
    "    \n",
    "    def select_action_model(self, state):\n",
    "        with torch.no_grad():\n",
    "            if use_cuda:\n",
    "                inpu = Variable(state).cuda()\n",
    "            else:\n",
    "                inpu = Variable(state)\n",
    "                \n",
    "            prob = self.policy_net(inpu)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # _, predicted = torch.max(qval.data, 1)\n",
    "            # print(\"Predicted : \"+str(qval.data))\n",
    "            # action = predicted[0] # + 1\n",
    "            # print(action)\n",
    "            \n",
    "            \n",
    "            return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f7dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
